# 기본 개념

- 조건부 확률
- 마르코프 속성
  - 과거 일은 무시하고, 현재 상태 기준으로만 미래를 예측 (ex. 브라운 운동)

- 마르코프 연쇄
  - 마르코프 속성을 지닌 시스템의 시간에 따른 상태 변화
  - 이산적이면 `마르코프 연쇄`, 연속적이면 `마르코프 과정`
  - 상태 집합(S) : 상태의 집합 (날씨를 기준으로 할 때 맑음, 강우)
  - 상태 전이 매트릭스(P) : 각 상태별 확률을 (S x S) 매트릭스로 표현한 것
  - 단위시간이 t일 때, 3t 이후의 상태별 확률을 알고 싶으면 P^3 하면 됨
- 환경 (Enviroment)
  - 에피소드들의 집합
- 에피소드 (Episode)
  - 일련의 연속된 상태 변화
- 확률의 기댓값
- 마르코프 보상 과정 (MRP)
  - 상태 집합 S, 상태 전이 매트릭스 P, 보상 함수 R, 감가율 gamma
  - 감가율
    - 시간의 흐름에 따라 가치를 얼마의 비율로 할인할 지를 결정하는 비율
  - 보상함수 R
    - $R_s = E[R_{t+1} | S_t = s]$
    - 시간 t에서 상태 s일 때 시간 t+1의 기댓값
    - 각 (t, s)일 때의 보상함수 값은 정해져 있음 (R의 인자는 t, s)
  - 반환값 (G)
    - 시간 t에서 계산한 누적 보상의 합계
    - 에피소드 별 반환값을 계산해서 에피소드의 효율성을 계산
    - $G_t = R_{t+1} + R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$
  - 상태 가치 함수 v
    - 환경(모든 에피소드) 전체에 대한 가치 측정 (에피소드별 가치 측정은 G로)
    - $v(s) = E[G_t | S_t=s] = ... = E[R_{t+1} + \gamma G_{t+1} | S_{t+1} = s]= E[R_{t+1} + \gamma v(s_{t+1}) | S_t = s]$
    - v(s)는 상태 s일 때 반환값의 기댓값
    - 벨만 방정식 
      - $v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = R_{t+1} + \gamma E[v(S_{t+1}) | S_t = s] = R_{t+1} + \gamma \sum_{s' \in S} P_{ss'} v(s')$
        - $P_{ss'}$ 는 s에서 s'으로 갈 때의 확률 (상태 전이 매트릭스에서 `P[s][s']`)





# 기본 알고리즘



## 마르코프 결정 과정 MDP

마르코프 결정과정 (MDP) 은 마르코프 보상 과정에 행동 A, 정책 $\pi$ 이 추가된 개념 (행동에 따라 상태 전이 매트릭스 P의 값이 바뀜)

MDP = MRP + action + policy

환경의 가치를 극대화하는 정책을 결정하는 것



행동의 집합 A

- 다음 상태에 영향을 미치는 행위
- 상태 전이 매트릭스 P는 다음과 같이 표현. $P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]$

 정책 ($\pi$, policy)

- 행동을 선택할 확률
- $\pi = P[A_t = a | S_t = s]$



즉, 액션이 정해지면, 그 액션에 따라 다음 상태 s1, s2로 갈 확률이 정해진다. (MRP에는 액션을 고려하지 않고, 그냥 s1, s2로 갈 확률이 정해져 있음 (상태 전이 매트릭스 P))



$P_{ss'}^{\pi} = \sum_{a\in A} \pi (a | s) P_{ss'}^a$

$R_s^\pi = \sum _{a\in A} \pi (a|s) R_s^a$



행동가치함수 Q

- 행동에 따른 가치를 평가하는 함수
- 정책을 평가하기 위해, 행동에 따른 가치를 평가하는 함수
- 선택할 수 있는 여러 행동 중 하나를 선택했을 때의 가치
- $q_\pi (s, a) = E_\pi [R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a] = R_s^a + \gamma \sum_{s' \in S} + \gamma \sum_{s' \in S} P_{ss'} ^a \pi(s', a') q_\pi (s', a')$
- 행동가치함수 q와 상태가치함수 v의 관계
  - $v_\pi (s) = \sum _{a\in A} \pi(a|s) q_\pi (s, a)$
  - $q_\pi (s,a) = R_s^a + \gamma \sum_{s' \in S} P_{s s'} ^a v_{\pi} (s')$



최적정책

- 최적의 가치를 얻도록 행동할 수 있게 만드는 정책
- 정책이란 행동을 선택할 수 있는 확률이기 때문에, 값이 크다는 얘기는 확률이 높다는 얘기









GD, SGD 표기법

편미분 : $\nabla _w J(w) = ({\part J(w) \over \part w_1}, ..., {\part J(w) \over \part w_n})$

경사하강법 : $\Delta w = - {1 \over 2} \propto \nabla _w J(w)$





# DQN 알고리즘

- 가치함수 대신 Q 함수를 사용
- 행동가치함수를 인공신경망을 사용해서 표현하는 방법 (Deep Q Learning)
- 기존 : 상태가치함수, 행동가치함수가 배열로 나타남
- 현실 : 배열로 표현 x => 인공신경망으로 해결해야 함. 이 때 가중치 w가 필요 (에피소드를 통해서 가치함수를 얻을 수 있다.)
- 인공신경망으로 경사하강법을 사용해 MDP를 구한다?



### 기본 개념

- 가치기반 강화학습
  - 인공신경망의 학습 대상이 가치
  - 가치함수를 기반으로 정책 결정 (DQN, MC, TD). 참가치함수를 목표로 함
- 정책기반 강화학습
  - 인공신경망의 학습 대상이 정책
  - 정책을 직접 학습 (REINFORCE, A2C, PPO)



개념

- 탐험

  - 에이전트가 다양한 경험을 하는 것

- 탐욕 

  - 에이전트가 누적 보상이 최대가 되도록 행동하는 것

- 탐험과 탐욕의 문제

  - 정책이 불완전한 학습 초기에 탐욕 정책에 따라 행동하면 다른 좋은 상태를 경험할 수 없는 문제

- 입실론 탐욕 정책

  - 에이전트가 탐욕 정책을 사용하면서 다양한 상태를 탐험하도록 하는 기법

  - 입실론 값은 시간이 지남에 따라 줄어든다.

  - ```
    if randomValue > epsilon:
    	argmax(a)
    else:
    	random(a)
    ```

- 리플레이 메모리 (재현 메모리)

  - 시간적 연관성이 높을수록 성능이 떨어짐 (한 번 미로의 막다른 길에 들어서면, 꽤나 과거의 지점까지 돌아와야 탈출 가능하다)
  - 에이전트가 탐험을 통해 리플레이 메모리 생성
  - 리플레이 메모리 생성 과정이 끝나면 리플레이 메모리에서 데이터를 추출해 모델을 학습
  - 학습 데이터가 시간적인 상관관계를 없애주는 역할 (과거 데이터로 모델 학습이 가능)



### 알고리즘 프로그래밍

